{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyJslkQANXLXpJ5ckmB+AH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AimanDzaky/Dzaky-projects/blob/main/AI_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLNA9L_jqHLR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n",
        "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Load dataset into DataFrame\n",
        "data = pd.read_csv(url, header=0, names=column_names)\n",
        "\n",
        "# Drop any empty rows (if any)\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L2Msm-dWsMu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode labels\n",
        "# Map class labels to integers (e.g., 'Iris-setosa' = 0, 'Iris-versicolor' = 1, 'Iris-virginica' = 2)\n",
        "class_mapping = {label: idx for idx, label in enumerate(data['class'].unique())}\n",
        "data['class'] = data['class'].map(class_mapping)\n",
        "\n",
        "# Shuffle the dataset to ensure random distribution across train/test splits\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Extract features and labels\n",
        "X = data.drop('class', axis=1).values\n",
        "y = data['class'].values\n",
        "\n",
        "# Normalize (Z-score)\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X = (X - X_mean) / X_std\n",
        "\n",
        "# Add bias term by appending a column of ones to the features\n",
        "X = np.hstack((X, np.ones((X.shape[0], 1))))"
      ],
      "metadata": {
        "id": "z6aeiAp4s2Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to split dataset into train and test sets based on a given ratio\n",
        "def split_dataset(X, y, train_ratio):\n",
        "    split_index = int(len(X) * train_ratio)\n",
        "    return X[:split_index], y[:split_index], X[split_index:], y[split_index:]\n",
        "\n",
        "# Dataset splits\n",
        "splits = {\n",
        "    \"80_20\": split_dataset(X, y, 0.8),\n",
        "    \"70_30\": split_dataset(X, y, 0.7),\n",
        "    \"60_40\": split_dataset(X, y, 0.6)\n",
        "}"
      ],
      "metadata": {
        "id": "s32s9WI7_TH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights (zero or small random values)\n",
        "def init_weights(n_classes, n_features, strategy='zeros'):\n",
        "    if strategy == 'zeros':\n",
        "        return np.zeros((n_classes, n_features))\n",
        "    elif strategy == 'random':\n",
        "        # Scaled random init\n",
        "        return np.random.randn(n_classes, n_features) * 0.01 # small random init\n",
        "\n",
        "# Train the Perceptron model until convergence or max_epochs\n",
        "def train_perceptron(X, y, n_classes, learning_rate=0.1, init='zeros', max_epochs=10000):\n",
        "    n_features = X.shape[1]\n",
        "    W = init_weights(n_classes, n_features, strategy=init)\n",
        "    for epoch in range(max_epochs):\n",
        "        errors = 0\n",
        "        for i in range(len(X)):\n",
        "            xi = X[i]\n",
        "            yi = y[i]\n",
        "            scores = np.dot(W, xi)\n",
        "            pred = np.argmax(scores)\n",
        "            # Update weights if prediction is wrong\n",
        "            if pred != yi:\n",
        "                W[yi] += learning_rate * xi\n",
        "                W[pred] -= learning_rate * xi\n",
        "                errors += 1\n",
        "        if errors == 0:\n",
        "            break         # Model has converged\n",
        "    return W, epoch + 1   # Return trained weights and number of epochs used\n",
        "\n",
        "# Predict class labels using learned weight\n",
        "def predict(X, W):\n",
        "    return np.argmax(np.dot(X, W.T), axis=1)\n",
        "\n",
        "# Compute classification accuracy\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# Build confusion matrix manually (no sklearn used)\n",
        "def confusion_matrix(y_true, y_pred, n_classes):\n",
        "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
        "    for actual, predicted in zip(y_true, y_pred):\n",
        "        cm[actual][predicted] += 1\n",
        "    return cm\n",
        "\n",
        "# Run experiments with confusion matrix\n",
        "def run_experiments(splits):\n",
        "    learning_rates = [0.01, 0.1]\n",
        "    inits = ['zeros', 'random']\n",
        "    n_classes = 3\n",
        "\n",
        "    for split_name, (X_train, y_train, X_test, y_test) in splits.items():\n",
        "        print(f\"\\n===== Split: {split_name.replace('_', '/')} =====\")\n",
        "\n",
        "        for lr in learning_rates:\n",
        "            for init in inits:\n",
        "                print(f\"\\n[LR={lr}, Init={init}]\")\n",
        "                W, epochs = train_perceptron(X_train, y_train, n_classes=n_classes, learning_rate=lr, init=init)\n",
        "                y_pred = predict(X_test, W)\n",
        "                acc = accuracy(y_test, y_pred)\n",
        "                cm = confusion_matrix(y_test, y_pred, n_classes=n_classes)\n",
        "\n",
        "                print(f\" → Epochs: {epochs}, Accuracy: {acc:.4f}\")\n",
        "                print(\"Confusion Matrix:\")\n",
        "                print(cm)\n",
        "\n",
        "\n",
        "# Run\n",
        "run_experiments(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBm2HCJA_eDR",
        "outputId": "41522b69-97c2-46e6-fc74-9f40f83584fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Split: 80/20 =====\n",
            "\n",
            "[LR=0.01, Init=zeros]\n",
            " → Epochs: 10000, Accuracy: 1.0000\n",
            "Confusion Matrix:\n",
            "[[ 7  0  0]\n",
            " [ 0 11  0]\n",
            " [ 0  0 12]]\n",
            "\n",
            "[LR=0.01, Init=random]\n",
            " → Epochs: 10000, Accuracy: 0.9667\n",
            "Confusion Matrix:\n",
            "[[ 7  0  0]\n",
            " [ 0 11  0]\n",
            " [ 0  1 11]]\n",
            "\n",
            "[LR=0.1, Init=zeros]\n",
            " → Epochs: 10000, Accuracy: 1.0000\n",
            "Confusion Matrix:\n",
            "[[ 7  0  0]\n",
            " [ 0 11  0]\n",
            " [ 0  0 12]]\n",
            "\n",
            "[LR=0.1, Init=random]\n",
            " → Epochs: 10000, Accuracy: 1.0000\n",
            "Confusion Matrix:\n",
            "[[ 7  0  0]\n",
            " [ 0 11  0]\n",
            " [ 0  0 12]]\n",
            "\n",
            "===== Split: 70/30 =====\n",
            "\n",
            "[LR=0.01, Init=zeros]\n",
            " → Epochs: 10000, Accuracy: 0.9556\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 17  0]\n",
            " [ 0  2 16]]\n",
            "\n",
            "[LR=0.01, Init=random]\n",
            " → Epochs: 10000, Accuracy: 0.9556\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 17  0]\n",
            " [ 0  2 16]]\n",
            "\n",
            "[LR=0.1, Init=zeros]\n",
            " → Epochs: 10000, Accuracy: 0.9556\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 17  0]\n",
            " [ 0  2 16]]\n",
            "\n",
            "[LR=0.1, Init=random]\n",
            " → Epochs: 10000, Accuracy: 0.9778\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 17  0]\n",
            " [ 0  1 17]]\n",
            "\n",
            "===== Split: 60/40 =====\n",
            "\n",
            "[LR=0.01, Init=zeros]\n",
            " → Epochs: 175, Accuracy: 0.9500\n",
            "Confusion Matrix:\n",
            "[[14  1  0]\n",
            " [ 0 19  2]\n",
            " [ 0  0 24]]\n",
            "\n",
            "[LR=0.01, Init=random]\n",
            " → Epochs: 123, Accuracy: 0.9833\n",
            "Confusion Matrix:\n",
            "[[15  0  0]\n",
            " [ 0 20  1]\n",
            " [ 0  0 24]]\n",
            "\n",
            "[LR=0.1, Init=zeros]\n",
            " → Epochs: 175, Accuracy: 0.9500\n",
            "Confusion Matrix:\n",
            "[[14  1  0]\n",
            " [ 0 19  2]\n",
            " [ 0  0 24]]\n",
            "\n",
            "[LR=0.1, Init=random]\n",
            " → Epochs: 199, Accuracy: 0.9667\n",
            "Confusion Matrix:\n",
            "[[14  1  0]\n",
            " [ 0 20  1]\n",
            " [ 0  0 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label encode\n",
        "    label_map = {label: idx for idx, label in enumerate(np.unique(y))}\n",
        "    y_encoded = np.array([label_map[label] for label in y])\n",
        "\n",
        "    # Normalize features\n",
        "    X_mean = X.mean(axis=0)\n",
        "    X_std = X.std(axis=0)\n",
        "    X_norm = (X - X_mean) / X_std\n",
        "\n",
        "    return X_norm, y_encoded\n",
        "\n",
        "# Split dataset into training and testing\n",
        "def split_data(X, y, train_ratio=0.8):\n",
        "    np.random.seed(42)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    train_size = int(train_ratio * len(X))\n",
        "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
        "    return X[train_idx], y[train_idx], X[test_idx], y[test_idx]\n",
        "\n",
        "# Initialize weights\n",
        "def init_weights(n_classes, n_features, strategy='zeros'):\n",
        "    if strategy == 'zeros':\n",
        "        return np.zeros((n_classes, n_features))\n",
        "    elif strategy == 'random':\n",
        "        return np.random.uniform(-0.5, 0.5, (n_classes, n_features))\n",
        "\n",
        "# Train perceptron until convergence\n",
        "def train_perceptron(X, y, n_classes, learning_rate=0.1, init='zeros', max_epochs=1000):\n",
        "    n_features = X.shape[1]\n",
        "    W = init_weights(n_classes, n_features, strategy=init)\n",
        "    converged = False\n",
        "    epoch = 0\n",
        "\n",
        "    while not converged and epoch < max_epochs:\n",
        "        errors = 0\n",
        "        for i in range(len(X)):\n",
        "            xi = X[i]\n",
        "            yi = y[i]\n",
        "            scores = np.dot(W, xi)\n",
        "            pred = np.argmax(scores)\n",
        "\n",
        "            if pred != yi:\n",
        "                W[yi] += learning_rate * xi\n",
        "                W[pred] -= learning_rate * xi\n",
        "                errors += 1\n",
        "\n",
        "        if errors == 0:\n",
        "            converged = True\n",
        "        epoch += 1\n",
        "\n",
        "    return W, epoch\n",
        "\n",
        "# Predict\n",
        "def predict(X, W):\n",
        "    return np.argmax(np.dot(X, W.T), axis=1)\n",
        "\n",
        "# Accuracy\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# Run experiment with settings\n",
        "def run_experiments():\n",
        "    X, y = load_data()\n",
        "    splits = [0.8, 0.7, 0.6]\n",
        "    learning_rates = [0.01, 0.1]\n",
        "    inits = ['zeros', 'random']\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\n===== Split: {int(split*100)}% Train / {int((1-split)*100)}% Test =====\")\n",
        "        X_train, y_train, X_test, y_test = split_data(X, y, train_ratio=split)\n",
        "\n",
        "        for lr in learning_rates:\n",
        "            for init in inits:\n",
        "                print(f\"\\n[LR={lr}, Init={init}]\")\n",
        "                W, epochs = train_perceptron(X_train, y_train, n_classes=3, learning_rate=lr, init=init)\n",
        "                y_pred = predict(X_test, W)\n",
        "                acc = accuracy(y_test, y_pred)\n",
        "                print(f\" → Epochs: {epochs}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Execute all experiments\n",
        "run_experiments()\n"
      ],
      "metadata": {
        "id": "DOo6w3q-r3me"
      }
    }
  ]
}